{"cells":[{"cell_type":"markdown","metadata":{"id":"OlxulJVRVg2n"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/juansensio/axr/blob/master/axr/00_intro.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"mYaNMwi2Vg2r"},"source":["# Introducción"]},{"cell_type":"markdown","metadata":{"id":"F8HDm-8VVg2t"},"source":["El aprendizaje por refuerzo explora una aproximación computacional al aprendizaje por interacción. De la misma manera que cuando aprendemos a conducir estamos atentos a cómo reacciona nuestro entorno a nuestras acciones y buscamos maneras de influenciarlo a través de nuestro comportamiento, el aprendizaje por refuerzo estudia cómo agentes computacionales pueden desarrollar comportamientos inteligentes a través de este tipo de interacción."]},{"cell_type":"markdown","metadata":{"id":"7wxSJeT_Vg2v"},"source":["## Aprendizaje por refuerzo\n"]},{"cell_type":"markdown","metadata":{"id":"AmdVMTo_Vg2w"},"source":["El aprendizaje por refuerzo, de ahora en adelante **axr**, consiste en aprender qué hacer (cómo relacionar situaciones y acciones) con el objetivo de maximizar una recompensa numérica. En ningún momento especificamos qué acciones debe tomar un agente, sino que le dejamos descubrir cuales son las que le darán una mayor recompensa. En la mayoría de situaciones, una acción no solo afectará a la recompensa inmediata, si no que tendrá un efecto en todas las situaciones futuras. Estas dos propiedades, búsqueda por prueba y error y futuras recompensas, son las más importantes del axr.\n","\n","Para formalizar el problema del axr utilizamos ideas del campo de la teoría de sistemas dinámicos, en concreto el control óptimo de procesos de Markov incompletos. La idea básica consiste en aprender los aspectos más importantes sobre el problema real al que nuestro agente se enfrenta a través de la interacción con el entrono para conseguir su objetivo. El agente tiene que ser capaz de precibir su entorno y de llevar a cabo acciones que afecten a su estado. También necesita uno o varios objetivos relacionados con el estado del entorno. Un proceso de decisión de Markov incluye estos tres aspectos: percepción, acción y objetivo. Cualquier método que sea capaz de resolver este tipo de problemas se considera como un método de axr.\n","\n","El axr está considerado como un paradigma del *machine learning* diferente al aprendizaje supervisado y no supervisado. Se diferencia del aprendizaje supervisado en que no siempre será posible obtener ejemplos del comportamiento deseado para nuestro agente en cualquier tipo de situación en la que se pueda encontrar, por lo que deberá ser capaz de aprender de su propia experiencia. Por otro lado, se diferencia del aprendizaje no supervisado ya que éste no es capaz por si mismo de resolver el problema de maximización de la recompensa.\n","\n","El principal problema al que nos enfrentamos en el axr es el balance entre **exploración** y **explotación**. Para obtener la máxima recompensa, un agente podría escoger aquellas acciones que ya conoce que y que le direon buenos resultados. Sin embargo, el hecho de explorar nuevas acciones podría, eventualmente, dar mucho mejor resultado. Así pues, nuestro agente tiene que ser capaz de explotar su conocimiento para obtener recompensa pero también de explorar para descubrir mejores acciones. El problema es que ninguna de las dos aproximaciones puede llevarse a cabo de manera independiente para resolver un problema. Un agente debe probar muchas acciones y, poco a poco, favorecer aquellas que parezcan ser mejores. Este problema sigue sin estar resuelto.\n","\n","Otra propiedad importante que diferencia al axr de otro métodos es su capacidad de considerar todo el dominio del problema de un agente interactuando con su entrono, y no pequeñas partes o sub-tareas que puedan resolverse de manera independiente para llevar al objetivo final."]},{"cell_type":"markdown","metadata":{"id":"nLv53bfRVg20"},"source":["## Elementos del axr\n"]},{"cell_type":"markdown","source":["Clase del Tablero (ConnectFourBoard)"],"metadata":{"id":"ZL1qeNWUTDnP"}},{"cell_type":"code","source":["import numpy as np\n","\n","class ConnectFourBoard():\n","    def __init__(self):\n","        self.state = np.zeros((6, 7))  # Tablero de 6 filas x 7 columnas\n","        self.current_player = 1\n","\n","    def valid_moves(self):\n","        \"\"\"Devuelve las columnas disponibles (0-6) donde se puede colocar ficha\"\"\"\n","        return [col for col in range(7) if self.state[0, col] == 0]\n","\n","    def update(self, symbol, col):\n","        \"\"\"Coloca una ficha en la columna especificada\"\"\"\n","        if col not in self.valid_moves():\n","            raise ValueError(\"Movimiento ilegal: columna llena o inválida!\")\n","\n","        # Encontrar la primera fila vacía en la columna\n","        for row in range(5, -1, -1):\n","            if self.state[row, col] == 0:\n","                self.state[row, col] = symbol\n","                break\n","\n","    def is_game_over(self):\n","        # Comprobar 4 en línea horizontal\n","        for row in range(6):\n","            for col in range(4):\n","                if abs(sum(self.state[row, col:col+4])) == 4:\n","                    return self.state[row, col]\n","\n","        # Comprobar 4 en línea vertical\n","        for col in range(7):\n","            for row in range(3):\n","                if abs(sum(self.state[row:row+4, col])) == 4:\n","                    return self.state[row, col]\n","\n","        # Comprobar diagonales (pendiente positiva)\n","        for row in range(3):\n","            for col in range(4):\n","                if abs(sum(self.state[row+i, col+i] for i in range(4))) == 4:\n","                    return self.state[row, col]\n","\n","        # Comprobar diagonales (pendiente negativa)\n","        for row in range(3, 6):\n","            for col in range(4):\n","                if abs(sum(self.state[row-i, col+i] for i in range(4))) == 4:\n","                    return self.state[row, col]\n","\n","        # Empate (tablero lleno)\n","        if len(self.valid_moves()) == 0:\n","            return 0\n","\n","        # El juego continúa\n","        return None\n","\n","    def reset(self):\n","        self.state = np.zeros((6, 7))\n","        self.current_player = 1\n","\n","    def print_board(self):\n","        \"\"\"Muestra el tablero de forma legible\"\"\"\n","        symbols = {0: '·', 1: 'X', -1: 'O'}\n","        print(\"\\n  0 1 2 3 4 5 6\")\n","        for row in self.state:\n","            print(\"  \" + \" \".join(symbols[cell] for cell in row))\n","        print()"],"metadata":{"id":"jq18u9qkHpRC","executionInfo":{"status":"ok","timestamp":1748322694181,"user_tz":240,"elapsed":38,"user":{"displayName":"Ovando Jesús Adrián","userId":"08198675745150551821"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Clase del Juego (ConnectFourGame)"],"metadata":{"id":"GjUoq4QzTIVC"}},{"cell_type":"code","execution_count":4,"metadata":{"id":"_sH9G9bWVg25","executionInfo":{"status":"ok","timestamp":1748322708272,"user_tz":240,"elapsed":4,"user":{"displayName":"Ovando Jesús Adrián","userId":"08198675745150551821"}}},"outputs":[],"source":["from tqdm import tqdm\n","\n","class ConnectFourGame():\n","    def __init__(self, player1, player2):\n","        player1.symbol = 1\n","        player2.symbol = -1\n","        self.players = [player1, player2]\n","        self.board = ConnectFourBoard()\n","\n","    def selfplay(self, rounds=100):\n","        wins = [0, 0]\n","        for i in tqdm(range(1, rounds + 1)):\n","            self.board.reset()\n","            for player in self.players:\n","                player.reset()\n","\n","            game_over = False\n","            while not game_over:\n","                for player in self.players:\n","                    col = player.move(self.board)\n","                    self.board.update(player.symbol, col)\n","\n","                    for p in self.players:\n","                        if hasattr(p, 'update'):\n","                            p.update(self.board)\n","\n","                    if self.board.is_game_over() is not None:\n","                        game_over = True\n","                        break\n","\n","            self.reward()\n","\n","            winner = self.board.is_game_over()\n","            for ix, player in enumerate(self.players):\n","                if winner == player.symbol:\n","                    wins[ix] += 1\n","\n","        return wins\n","\n","    def reward(self):\n","        winner = self.board.is_game_over()\n","        if winner == 0:  # empate\n","            for player in self.players:\n","                player.reward(0.5)\n","        else:\n","            for player in self.players:\n","                if winner == player.symbol:\n","                    player.reward(1)\n","                else:\n","                    player.reward(0)"]},{"cell_type":"markdown","source":["Clase del Agente (Agent)"],"metadata":{"id":"icgk2wNkTME0"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"4KLg97K6Vg25","executionInfo":{"status":"ok","timestamp":1748322721023,"user_tz":240,"elapsed":56,"user":{"displayName":"Ovando Jesús Adrián","userId":"08198675745150551821"}}},"outputs":[],"source":["import numpy as np\n","\n","class Agent():\n","    def __init__(self, alpha=0.5, prob_exp=0.5):\n","        self.value_function = {}  # Diccionario estado -> valor\n","        self.alpha = alpha        # Tasa de aprendizaje\n","        self.positions = []       # Historial de posiciones en la partida\n","        self.prob_exp = prob_exp  # Probabilidad de exploración\n","        self.symbol = None        # Se establecerá al iniciar el juego\n","\n","    def reset(self):\n","        self.positions = []\n","\n","    def move(self, board, explore=True):\n","        valid_moves = board.valid_moves()\n","\n","        # Exploración: movimiento aleatorio\n","        if explore and np.random.uniform(0, 1) < self.prob_exp:\n","            return np.random.choice(valid_moves)\n","\n","        # Explotación: elegir mejor movimiento según value_function\n","        max_value = -float('inf')\n","        best_col = valid_moves[0]  # Por defecto, primera columna válida\n","\n","        for col in valid_moves:\n","            # Simular el movimiento\n","            next_board = board.state.copy()\n","            for row in range(5, -1, -1):\n","                if next_board[row, col] == 0:\n","                    next_board[row, col] = self.symbol\n","                    break\n","\n","            # Obtener valor del estado resultante\n","            state_key = str(next_board.reshape(6*7))\n","            value = self.value_function.get(state_key, 0)\n","\n","            if value > max_value:\n","                max_value = value\n","                best_col = col\n","\n","        return best_col\n","\n","    def update(self, board):\n","        self.positions.append(str(board.state.reshape(6*7)))\n","\n","    def reward(self, reward):\n","        # Actualizar value_function basado en recompensa\n","        for state in reversed(self.positions):\n","            if state not in self.value_function:\n","                self.value_function[state] = 0\n","            self.value_function[state] += self.alpha * (reward - self.value_function[state])\n","            reward = self.value_function[state]"]},{"cell_type":"markdown","source":["Entrenamiento de los Agentes"],"metadata":{"id":"L1T_fhH_TP73"}},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jh9S5GThVg26","executionInfo":{"status":"ok","timestamp":1748323060915,"user_tz":240,"elapsed":325817,"user":{"displayName":"Ovando Jesús Adrián","userId":"08198675745150551821"}},"outputId":"68fab39c-0e2c-453f-f82c-769dc5f6570f"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10000/10000 [05:25<00:00, 30.69it/s]"]},{"output_type":"stream","name":"stdout","text":["Resultados del entrenamiento: [5915, 4027]\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# Crear agentes\n","agent1 = Agent(prob_exp=0.5)  # Mayor exploración inicial\n","agent2 = Agent(prob_exp=0.3)  # Menos exploración\n","\n","# Crear juego\n","game = ConnectFourGame(agent1, agent2)\n","\n","# Entrenamiento (puedes reducir las rondas para probar)\n","results = game.selfplay(10000)  # Usar menos rondas para prueba inicial\n","print(\"Resultados del entrenamiento:\", results)"]},{"cell_type":"markdown","source":["Visualización de la Función de Valor"],"metadata":{"id":"ocfjvi2hTTiS"}},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":352},"id":"-BrBIuh-Vg26","executionInfo":{"status":"ok","timestamp":1748323080797,"user_tz":240,"elapsed":485,"user":{"displayName":"Ovando Jesús Adrián","userId":"08198675745150551821"}},"outputId":"5eb6f69a-5d2b-4ecd-f80c-77f6cb03b585"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                              Estado  Valor\n","0  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  ...    1.0\n","1  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  ...    1.0\n","2  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  ...    1.0\n","3  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  ...    1.0\n","4  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  ...    1.0\n","5  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  ...    1.0\n","6  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  ...    1.0\n","7  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  ...    1.0\n","8  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  ...    1.0\n","9  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  ...    1.0"],"text/html":["\n","  <div id=\"df-559379a5-92d0-449e-8498-70d5984d7ff7\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Estado</th>\n","      <th>Valor</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  ...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  ...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  ...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  ...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  ...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  ...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  ...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  ...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  ...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  ...</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-559379a5-92d0-449e-8498-70d5984d7ff7')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-559379a5-92d0-449e-8498-70d5984d7ff7 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-559379a5-92d0-449e-8498-70d5984d7ff7');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-514d6ccd-4ca6-4774-bbea-cb9b4cad797a\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-514d6ccd-4ca6-4774-bbea-cb9b4cad797a')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-514d6ccd-4ca6-4774-bbea-cb9b4cad797a button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","  <div id=\"id_a1413448-c193-4c4e-90ed-f385d52dd6d7\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('value_df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_a1413448-c193-4c4e-90ed-f385d52dd6d7 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('value_df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"value_df","summary":"{\n  \"name\": \"value_df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"Estado\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -1.  0.  0.  0.  0. -1.\\n  0. -1.  1.  1.  1.  1.]\",\n          \"[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\\n  0.  0.  0. -1.  0.  0.  0.  0.  0.  0. -1.  0.  0.  0.  0.  0.  0. -1.\\n  0.  0.  1.  1.  1.  1.]\",\n          \"[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -1.  0.  0.  0.  0.  0.  0. -1.\\n -1.  1.  1.  1.  1.  0.]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Valor\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.1779365105939119e-09,\n        \"min\": 0.9999999962747097,\n        \"max\": 1.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          1.0,\n          0.9999999999999999,\n          0.999999999998181\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":7}],"source":["import pandas as pd\n","\n","# Ordenar estados por valor\n","sorted_values = sorted(agent1.value_function.items(), key=lambda x: x[1], reverse=True)\n","\n","# Crear DataFrame para visualización\n","value_df = pd.DataFrame({\n","    'Estado': [x[0] for x in sorted_values[:10]],  # Mostrar solo los 10 mejores\n","    'Valor': [x[1] for x in sorted_values[:10]]\n","})\n","\n","value_df"]},{"cell_type":"markdown","source":["Guardar el Agente Entrenado"],"metadata":{"id":"CigWsZo8TWsD"}},{"cell_type":"code","execution_count":8,"metadata":{"id":"kM3UDgoZVg27","executionInfo":{"status":"ok","timestamp":1748323117488,"user_tz":240,"elapsed":73,"user":{"displayName":"Ovando Jesús Adrián","userId":"08198675745150551821"}}},"outputs":[],"source":["import pickle\n","\n","with open('connect4_agent.pickle', 'wb') as f:\n","    pickle.dump(agent1.value_function, f)"]},{"cell_type":"markdown","metadata":{"id":"AMo7ICQ-Vg27"},"source":["Este ejemplo sirve para ilustrar algunas de las propiedades clave del axr. En primer lugar, aprender a través de la interacción con el entorno (en este caso el otro agente). En segundo lugar, tenemos un objetivo claro y el comportamiento correcto del agente requiere de planificación y predicción que tenga en cuenta los efectos futuros de sus acciones."]},{"cell_type":"markdown","metadata":{"id":"iXGGVZfgVg27"},"source":["##  Resumen"]},{"cell_type":"markdown","metadata":{"id":"MEx4YqHFVg28"},"source":["El aprendizaje por refuerzo es una aproximación computacional a la comprensión y automatización del aprendizaje por objetivos y toma de decisiones. En esta aproximación, una agente aprende a través de la interacción directa con su entorno sin necesidad de supervisión explícita. Utiliza procesos de decisión de Markov para definir la interacción entre el agente y su entorno en términos de estados, acciones y recompensas. Los conceptos de valor y función de valor son la clave de muchos métodos de axr ya que representan una manera eficiente de búsqueda en el espacio de políticas."]}],"metadata":{"colab":{"provenance":[{"file_id":"1hE8tqzYtNIJgVt5rAeARj9ZMmLve-mwI","timestamp":1748321823399},{"file_id":"https://github.com/juansensio/axr/blob/master/axr/00_intro.ipynb","timestamp":1715661008364}]},"interpreter":{"hash":"bb9f406c0f70fca9801e60f2cbb7cd1ccff2ae2f74c58f513340bcf6cae5ecd0"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"}},"nbformat":4,"nbformat_minor":0}